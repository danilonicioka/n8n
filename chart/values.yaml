# README
# High level values structure, overview and explanation of the values.yaml file.
# 1. Global and chart wide values, like the image repository, image tag, etc.
# 2. Ingress, (default is nginx, but you can change it to your own ingress controller)
# 3. Main n8n app configuration + kubernetes specific settings
# 4. Worker related settings + kubernetes specific settings
# 5. Webhook related settings + kubernetes specific settings
# 6. Raw Resources to pass through your own manifests like GatewayAPI, ServiceMonitor etc.
# 7. Valkey/Redis related settings and kubernetes specific settings

#
# Global common config for this entire n8n deployment
#

image:
  repository: n8nio/n8n
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "stable"
imagePullSecrets: []

# The Name to use for the chart. Will be the prefix of all resources aka. The Chart.Name (default is 'n8n')
nameOverride:
# Override the full name of the deployment. When empty, the name will be "{release-name}-{chart-name}" or the value of nameOverride if specified
fullnameOverride:

# Add entries to a pod's /etc/hosts file, mapping custom IP addresses to hostnames.
hostAliases: []
  # - ip: 8.8.8.8
  #   hostnames:
  #     - service-example.local
#
# Ingress
#
ingress:
  enabled: true
  annotations:
    # Configurações básicas
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/backend-protocol: "HTTP"
    
    # Timeouts
    nginx.ingress.kubernetes.io/proxy-body-size: "100m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"
    
    # WebSockets
    nginx.ingress.kubernetes.io/proxy-http-version: "1.1"
    nginx.ingress.kubernetes.io/proxy-set-header: "Upgrade $http_upgrade"
    nginx.ingress.kubernetes.io/proxy-set-header: "Connection $connection_upgrade"
    
    # Headers de proxy
    nginx.ingress.kubernetes.io/proxy-set-header: "X-Real-IP $remote_addr"
    nginx.ingress.kubernetes.io/proxy-set-header: "X-Forwarded-For $proxy_add_x_forwarded_for"
    nginx.ingress.kubernetes.io/proxy-set-header: "X-Forwarded-Proto $scheme"
    nginx.ingress.kubernetes.io/proxy-set-header: "X-Forwarded-Host $host"
    
    # Confiança de proxy
    nginx.ingress.kubernetes.io/enable-real-ip: "true"
    nginx.ingress.kubernetes.io/use-forwarded-headers: "true"
    
    # SSL
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
  # define a custom ingress class Name, like "traefik" or "nginx"
  className: "nginx"
  hosts:
    - host: example.com
      paths:
        - path: /
          pathType: Prefix
  tls: {}
    # - hosts:
    #    - example.com
    #  secretName: host-domain-cert

# the main (n8n) application related configuration + Kubernetes specific settings
# The config: {} dictionary is converted to environmental variables in the ConfigMap.
main:
  # See https://docs.n8n.io/hosting/configuration/environment-variables/ for all values.
  config: 
    n8n:
      log:
        level: "debug"
      host: "example.com"                # N8N_HOST
      port: "5678"                                # N8N_PORT
      protocol: "https"                           # N8N_PROTOCOL
      secure:
        cookie: "true"                       # N8N_SECURE_COOKIE
      diagnostics:
        enabled: "false"                # N8N_DIAGNOSTICS_ENABLED
      enforce:
        settings:
          file:
            permissions: "true"              # N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS
      block:
        env:
          access:
            in:
              node: "false"                      # N8N_BLOCK_ENV_ACCESS_IN_NODE
      git:
        node:
          disable:
            bare:
              repos: "true"                    # N8N_GIT_NODE_DISABLE_BARE_REPOS
      email:
        mode: "smtp"                                           # N8N_EMAIL_MODE
      smtp:
        host: ""                             # N8N_SMTP_HOST
        port: ""                                             # N8N_SMTP_PORT
        user: ""                      # N8N_SMTP_USER
        sender: ""                         # N8N_SMTP_SENDER
        ssl: "false"                                           # N8N_SMTP_SSL
        starttls: "false"                                       # N8N_SMTP_STARTTLS
      runners:
        enabled: "true"
    webhook:
      url: "https://example.com" # WEBHOOK_URL
    generic:
      timezone: "America/Sao_Paulo"               # GENERIC_TIMEZONE
    db:
      type: "postgresdb"                        # DB_TYPE
      postgresdb:
        host: n8n-postgresql        # DB_POSTGRESDB_HOST
        port: "5432"                              # DB_POSTGRESDB_PORT
        database: "n8n"                          # DB_POSTGRESDB_DATABASE
        user: "n8n"                      # DB_POSTGRESDB_USER
    executions:
      mode: "queue"                             # EXECUTIONS_MODE
    offload:
      manual:
        executions:
          to:
            workers: "true"         # OFFLOAD_MANUAL_EXECUTIONS_TO_WORKERS
    queue:
      health:
        check:
          active: "true"
      bull:
        redis:
          username: "default"
          timeout:
            threshold: 30000
          cluster:
            nodes: "n8n-redis-0.n8n-redis-headless.n8n.svc.cluster.local:6379,n8n-redis-1.n8n-redis-headless.n8n.svc.cluster.local:6379,n8n-redis-2.n8n-redis-headless.n8n.svc.cluster.local:6379"
          host: "n8n-redis-0" # QUEUE_BULL_REDIS_HOST
          port: "6379"                                     # QUEUE_BULL_REDIS_PORT

  # Dictionary for secrets, unlike config:, the values here will end up in the secret file.
  # The YAML entry db.postgresdb.password: my_secret is transformed DB_POSTGRESDB_password=bXlfc2VjcmV0
  # See https://docs.n8n.io/hosting/configuration/environment-variables/
  secret:
    db:
      postgresdb:
        password: "password"
    n8n:
      smtp:
        pass: ""
      encryption:
        key: "key"
    queue:
      bull:
        redis:
          password: "password"
  #    n8n:
  #     if you run n8n stateless, you should provide an encryption key here.
  #      encryption_key:
  #
  #    db:
  #      postgresdb:
  #        password: 'big secret'

  # Extra environmental variables, so you can reference other configmaps and secrets into n8n as env vars.
  extraEnv:
    DB_POSTGRESDB_PASSWORD:
      valueFrom:
        secretKeyRef:
          name: n8n-app-secret
          key: DB_POSTGRESDB_PASSWORD
    QUEUE_BULL_REDIS_PASSWORD:
      valueFrom:
        secretKeyRef:
          name: n8n-app-secret
          key: QUEUE_BULL_REDIS_PASSWORD
    N8N_ENCRYPTION_KEY:
      valueFrom:
        secretKeyRef:
          name: n8n-app-secret
          key: N8N_ENCRYPTION_KEY
    N8N_SMTP_PASS:
      valueFrom:
        secretKeyRef:
          name: n8n-app-secret
          key: N8N_SMTP_PASS
  #    N8N_DB_POSTGRESDB_NAME:
  #      valueFrom:
  #        secretKeyRef:
  #          name: db-app
  #          key: dbname
  #
  # N8n Kubernetes specific settings
  #
  persistence:
    # If true, use a Persistent Volume Claim, If false, use emptyDir
    enabled: true
    # what type volume, possible options are [existing, emptyDir, dynamic] dynamic for Dynamic Volume Provisioning, existing for using an existing Claim
    type: emptyDir
    # Persistent Volume Storage Class
    # If defined, storageClassName: <storageClass>
    # If set to "-", storageClassName: "", which disables dynamic provisioning
    # If undefined (the default) or set to null, no storageClassName spec is
    #   set, choosing the default provisioner.  (gp2 on AWS, standard on
    #   GKE, AWS & OpenStack)
    #
    # storageClass: "-"
    # PVC annotations
    #
    # If you need this annotation include it under `values.yml` file and pvc.yml template will add it.
    # This is not maintained at Helm v3 anymore.
    # https://github.com/8gears/n8n-helm-chart/issues/8
    #
    # annotations:
    #   helm.sh/resource-policy: keep
    # Persistent Volume Access Mode
    #
    accessModes:
      - ReadWriteOnce
    # Persistent Volume size
    size: 20Gi
    # Use an existing PVC
    # existingClaim:

  extraVolumes: []
  #    - name: db-ca-cert
  #      secret:
  #        secretName: db-ca
  #        items:
  #          - key: ca.crt
  #            path: ca.crt

  extraVolumeMounts: []
  #    - name: db-ca-cert
  #      mountPath: /etc/ssl/certs/postgresql
  #      readOnly: true


  # Number of desired pods. More than one pod is supported in n8n enterprise.
  replicaCount: 1

  # here you can specify the deployment strategy as Recreate or RollingUpdate with optional maxSurge and maxUnavailable
  # If these options are not set, default values are 25%
  # deploymentStrategy:
  #  type: Recreate | RollingUpdate
  #  maxSurge: "50%"
  #  maxUnavailable: "50%"

  deploymentStrategy:
    type: "Recreate"
    #  maxSurge: "50%"
    #  maxUnavailable: "50%"

  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""

  # Annotations to be implemented on the main service deployment
  deploymentAnnotations: {}
  # Labels to be implemented on the main service deployment
  deploymentLabels: {}
  # Annotations to be implemented on the main service pod
  podAnnotations: {}
  # Labels to be implemented on the main service pod
  podLabels: {}

  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000

  securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  #  runAsNonRoot: true
  #  runAsUser: 1000

  # here you can specify lifecycle hooks - it can be used e.g., to easily add packages to the container without building
  # your own docker image
  # see https://github.com/8gears/n8n-helm-chart/pull/30
  lifecycle: {}

  #  here's the sample configuration to add mysql-client to the container
  # lifecycle:
  #  postStart:
  #    exec:
  #      command: ["/bin/sh", "-c", "apk add mysql-client"]

  # here you can override a command for main container
  # it may be used to override a starting script (e.g., to resolve issues like https://github.com/n8n-io/n8n/issues/6412) or run additional preparation steps (e.g., installing additional software)
  command: []

  # sample configuration that overrides starting script and solves above issue (also it runs n8n as root, so be careful):
  # command:
  #  - tini
  #  - --
  #  - /bin/sh
  #  - -c
  #  - chmod o+rx /root; chown -R node /root/.n8n || true; chown -R node /root/.n8n; ln -s /root/.n8n /home/node; chown -R node /home/node || true; node /usr/local/bin/n8n

  # here you can override the livenessProbe for the main container
  # it may be used to increase the timeout for the livenessProbe (e.g., to resolve issues like

  livenessProbe:
    httpGet:
      path: /healthz
      port: http
    # initialDelaySeconds: 30
    # periodSeconds: 10
    # timeoutSeconds: 5
    # failureThreshold: 6
    # successThreshold: 1

  # here you can override the readinessProbe for the main container
  # it may be used to increase the timeout for the readinessProbe (e.g., to resolve issues like

  readinessProbe:
    httpGet:
      path: /healthz
      port: http
    # initialDelaySeconds: 30
    # periodSeconds: 10
    # timeoutSeconds: 5
    # failureThreshold: 6
    # successThreshold: 1

  # List of initialization containers belonging to the pod. Init containers are executed in order prior to containers being started.
  # See https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
  initContainers: []
  #    - name: init-data-dir
  #      image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
  #      command: [ "/bin/sh", "-c", "mkdir -p /home/node/.n8n/" ]
  #      volumeMounts:
  #        - name: data
  #          mountPath: /home/node/.n8n


  service:
    enabled: true
    annotations: {}
    # -- Service types allow you to specify what kind of Service you want.
    # E.g., ClusterIP, NodePort, LoadBalancer, ExternalName
    type: ClusterIP
    # -- Service port
    port: 5678

  resources: {}
  # We usually recommend not specifying default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

  nodeSelector: {}
  tolerations: []
  affinity: {}

  # Pod termination grace period in seconds
  terminationGracePeriodSeconds: 30

# # # # # # # # # # # # # # # #
#
# Worker related settings
#
worker:
  enabled: true

  # additional (to main) config for worker
  config:
    n8n:
      log:
        level: "debug"
      diagnostics:
        enabled: "false"                           # N8N_DIAGNOSTICS_ENABLED
      enforce:
        settings:
          file:
            permissions: "true"                    # N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS
      block:
        env:
          access:
            in:
              node: "false"                        # N8N_BLOCK_ENV_ACCESS_IN_NODE
      git:
        node:
          disable:
            bare:
              repos: "true"                        # N8N_GIT_NODE_DISABLE_BARE_REPOS
      runners:
        enabled: "true"                            # N8N_RUNNERS_ENABLED

    generic:
      timezone: "America/Sao_Paulo"                # GENERIC_TIMEZONE

    db:
      type: "postgresdb"                           # DB_TYPE
      postgresdb:
        host: n8n-postgresql       # DB_POSTGRESDB_HOST
        port: "5432"                                # DB_POSTGRESDB_PORT
        database: "n8n"                          # DB_POSTGRESDB_DATABASE
        user: "n8n"   
        # For TLS you can add ssl_* keys here (only if required by your Postgres setup)

    executions:
      mode: "queue"                                 # EXECUTIONS_MODE

    offload:
      manual:
        executions:
          to:
            workers: "true"         # OFFLOAD_MANUAL_EXECUTIONS_TO_WORKERS
    queue:
      health:
        check:
          active: "true"
      bull:
        redis:
          username: "default"
          timeout:
            threshold: 30000
          cluster:
            nodes: "n8n-redis-0.n8n-redis-headless.n8n.svc.cluster.local:6379,n8n-redis-1.n8n-redis-headless.n8n.svc.cluster.local:6379,n8n-redis-2.n8n-redis-headless.n8n.svc.cluster.local:6379"         
          host: "n8n-redis-0" # QUEUE_BULL_REDIS_HOST
          port: "6379"                                     # QUEUE_BULL_REDIS_PORT 


  # additional (to main) config for worker
  secret: {}

  # Extra environmental variables, so you can reference other configmaps and secrets into n8n as env vars.
  extraEnv:
    DB_POSTGRESDB_PASSWORD:
      valueFrom:
        secretKeyRef:
          name: n8n-app-secret
          key: DB_POSTGRESDB_PASSWORD
    QUEUE_BULL_REDIS_PASSWORD:
      valueFrom:
        secretKeyRef:
          name: n8n-app-secret
          key: QUEUE_BULL_REDIS_PASSWORD
    N8N_ENCRYPTION_KEY:
      valueFrom:
        secretKeyRef:
          name: n8n-app-secret
          key: N8N_ENCRYPTION_KEY

  # Define the number of jobs a worker can run in parallel by using the concurrency flag. Default is 10
  concurrency: 10

  #
  # Worker Kubernetes specific settings
  #
  persistence:
    # If true, use a Persistent Volume Claim, If false, use emptyDir
    enabled: true
    # what type volume, possible options are [existing, emptyDir, dynamic] dynamic for Dynamic Volume Provisioning, existing for using an existing Claim
    type: emptyDir
    # Persistent Volume Storage Class
    # If defined, storageClassName: <storageClass>
    # If set to "-", storageClassName: "", which disables dynamic provisioning
    # If undefined (the default) or set to null, no storageClassName spec is
    #   set, choosing the default provisioner.  (gp2 on AWS, standard on
    #   GKE, AWS & OpenStack)
    #
    # storageClass: "-"
    # PVC annotations
    #
    # If you need this annotation include it under `values.yml` file and pvc.yml template will add it.
    # This is not maintained at Helm v3 anymore.
    # https://github.com/8gears/n8n-helm-chart/issues/8
    #
    # annotations:
    #   helm.sh/resource-policy: keep
    # Persistent Volume Access Mode
    accessModes:
      - ReadWriteOnce
    # Persistent Volume size
    size: 1Gi
    # Use an existing PVC
    # existingClaim:

  # Number of desired pods.
  replicaCount: 2

  # here you can specify the deployment strategy as Recreate or RollingUpdate with optional maxSurge and maxUnavailable
  # If these options are not set, default values are 25%
  # deploymentStrategy:
  #  type: RollingUpdate
  #  maxSurge: "50%"
  #  maxUnavailable: "50%"

  deploymentStrategy:
    type: "Recreate"
    # maxSurge: "50%"
    # maxUnavailable: "50%"

  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""

  # Annotations to be implemented on the worker deployment
  deploymentAnnotations: {}
  # Labels to be implemented on the worker deployment
  deploymentLabels: {}
  # Annotations to be implemented on the worker pod
  podAnnotations: {}
  # Labels to be implemented on the worker pod
  podLabels: {}

  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000

  securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  #  runAsNonRoot: true
  #  runAsUser: 1000

  # here you can specify lifecycle hooks - it can be used e.g., to easily add packages to the container without building
  # your own docker image
  # see https://github.com/8gears/n8n-helm-chart/pull/30
  lifecycle: {}

  #  here's the sample configuration to add mysql-client to the container
  # lifecycle:
  #  postStart:
  #    exec:
  #      command: ["/bin/sh", "-c", "apk add mysql-client"]

  # here you can override a command for worker container
  # it may be used to override a starting script (e.g., to resolve issues like https://github.com/n8n-io/n8n/issues/6412) or
  # run additional preparation steps (e.g., installing additional software)
  command: []

  # sample configuration that overrides starting script and solves above issue (also it runs n8n as root, so be careful):
  # command:
  #  - tini
  #  - --
  #  - /bin/sh
  #  - -c
  #  - chmod o+rx /root; chown -R node /root/.n8n || true; chown -R node /root/.n8n; ln -s /root/.n8n /home/node; chown -R node /home/node || true; node /usr/local/bin/n8n

  # command args
  commandArgs: []

  # here you can override the livenessProbe for the main container
  # it may be used to increase the timeout for the livenessProbe (e.g., to resolve issues like
  livenessProbe:
    httpGet:
      path: /healthz
      port: http
    # initialDelaySeconds: 30
    # periodSeconds: 10
    # timeoutSeconds: 5
    # failureThreshold: 6
    # successThreshold: 1

  # here you can override the readinessProbe for the main container
  # it may be used to increase the timeout for the readinessProbe (e.g., to resolve issues like

  readinessProbe:
    httpGet:
      path: /healthz
      port: http
    # initialDelaySeconds: 30
    # periodSeconds: 10
    # timeoutSeconds: 5
    # failureThreshold: 6
    # successThreshold: 1

  # List of initialization containers belonging to the pod. Init containers are executed in order prior to containers being started.
  # See https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
  initContainers: []

  service:
    annotations: {}
    # -- Service types allow you to specify what kind of Service you want.
    # E.g., ClusterIP, NodePort, LoadBalancer, ExternalName
    type: ClusterIP
    # -- Service port
    port: 80

  resources:
  # We usually recommend not specifying default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    limits:
      cpu: "1000m"
      memory: "512Mi"
    requests:
      cpu: "200m"
      memory: "256Mi"

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

  nodeSelector: {}
  tolerations: []
  affinity: {}

  # Pod termination grace period in seconds
  terminationGracePeriodSeconds: 30

# Webhook related settings
# With .Values.scaling.webhook.enabled=true you disable Webhooks from the main process, but you enable the processing on a different Webhook instance.
# See https://github.com/8gears/n8n-helm-chart/issues/39#issuecomment-1579991754 for the full explanation.
# Webhook processes rely on Valkey/Redis too.
webhook:
  enabled: true
  # additional (to main) config for webhook
  config: {}
  # additional (to main) config for webhook
  secret: {}

  # Extra environmental variables, so you can reference other configmaps and secrets into n8n as env vars.
  extraEnv:
    WEBHOOK_URL:
      value: "https://example.com"


  #
  # Webhook Kubernetes specific settings
  #
  persistence:
    # If true, use a Persistent Volume Claim, If false, use emptyDir
    enabled: false
    # what type volume, possible options are [existing, emptyDir, dynamic] dynamic for Dynamic Volume Provisioning, existing for using an existing Claim
    type: emptyDir
    # Persistent Volume Storage Class
    # If defined, storageClassName: <storageClass>
    # If set to "-", storageClassName: "", which disables dynamic provisioning
    # If undefined (the default) or set to null, no storageClassName spec is
    #   set, choosing the default provisioner.  (gp2 on AWS, standard on
    #   GKE, AWS & OpenStack)
    #
    # storageClass: "-"
    # PVC annotations
    #
    # If you need this annotation include it under `values.yml` file and pvc.yml template will add it.
    # This is not maintained at Helm v3 anymore.
    # https://github.com/8gears/n8n-helm-chart/issues/8
    #
    # annotations:
    #   helm.sh/resource-policy: keep
    # Persistent Volume Access Mode
    #
    accessModes:
      - ReadWriteOnce
    # Persistent Volume size
    #
    size: 1Gi
    # Use an existing PVC
    #
    # existingClaim:

  # Number of desired pods.
  replicaCount: 1

  # here you can specify the deployment strategy as Recreate or RollingUpdate with optional maxSurge and maxUnavailable
  # If these options are not set, default values are 25%
  # deploymentStrategy:
  #  type: RollingUpdate
  #  maxSurge: "50%"
  #  maxUnavailable: "50%"

  deploymentStrategy:
    type: "Recreate"

  nameOverride: ""
  fullnameOverride: ""

  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""

  # Annotations to be implemented on the webhook deployment
  deploymentAnnotations: {}
  # Labels to be implemented on the webhook deployment
  deploymentLabels: {}
  # Annotations to be implemented on the webhook pod
  podAnnotations: {}
  # Labels to be implemented on the webhook pod
  podLabels: {}

  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000

  securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  #  runAsNonRoot: true
  #  runAsUser: 1000

  # here you can specify lifecycle hooks - it can be used e.g., to easily add packages to the container without building
  # your own docker image
  # see https://github.com/8gears/n8n-helm-chart/pull/30
  lifecycle: {}

  #  here's the sample configuration to add mysql-client to the container
  # lifecycle:
  #  postStart:
  #    exec:
  #      command: ["/bin/sh", "-c", "apk add mysql-client"]

  # here you can override a command for main container
  # it may be used to override a starting script (e.g., to resolve issues like https://github.com/n8n-io/n8n/issues/6412) or
  # run additional preparation steps (e.g., installing additional software)
  command: []

  # sample configuration that overrides starting script and solves above issue (also it runs n8n as root, so be careful):
  # command:
  #  - tini
  #  - --
  #  - /bin/sh
  #  - -c
  #  - chmod o+rx /root; chown -R node /root/.n8n || true; chown -R node /root/.n8n; ln -s /root/.n8n /home/node; chown -R node /home/node || true; node /usr/local/bin/n8n
  # Command Arguments
  commandArgs: []

  # here you can override the livenessProbe for the main container
  # it may be used to increase the timeout for the livenessProbe (e.g., to resolve issues like

  livenessProbe:
    httpGet:
      path: /healthz
      port: http
    # initialDelaySeconds: 30
    # periodSeconds: 10
    # timeoutSeconds: 5
    # failureThreshold: 6
    # successThreshold: 1

  # here you can override the readinessProbe for the main container
  # it may be used to increase the timeout for the readinessProbe (e.g., to resolve issues like

  readinessProbe:
    httpGet:
      path: /healthz
      port: http
    # initialDelaySeconds: 30
    # periodSeconds: 10
    # timeoutSeconds: 5
    # failureThreshold: 6
    # successThreshold: 1

  # List of initialization containers belonging to the pod. Init containers are executed in order prior to containers being started.
  # See https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
  initContainers: []

  service:
    annotations: {}
    # -- Service types allow you to specify what kind of Service you want.
    # E.g., ClusterIP, NodePort, LoadBalancer, ExternalName
    type: ClusterIP
    # -- Service port
    port: 80

  resources: {}
  # We usually recommend not specifying default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80
  nodeSelector: {}
  tolerations: []
  affinity: {}

  # Pod termination grace period in seconds
  terminationGracePeriodSeconds: 30

#
# User defined supplementary K8s manifests
#

#  Takes a list of Kubernetes manifests and merges each resource with a default metadata.labels map and
#  installs the result.
#  Use this to add any arbitrary Kubernetes manifests alongside this chart instead of kubectl and scripts.
extraManifests: []
#  - apiVersion: v1
#    kind: ConfigMap
#    metadata:
#      name: example-config
#    data:
#      example.property.1: "value1"
#      example.property.2: "value2"
# As an alternative to the above, you can also use a string as the value of the data field.
#  - |
#    apiVersion: v1
#    kind: ConfigMap
#    metadata:
#      name: example-config-string
#    data:
#      example.property.1: "value1"
#      example.property.2: "value2"

# String extraManifests supports using variables directly within a string manifest.
# Templates are rendered using the context defined in the values.yaml file, enabling dynamic and flexible content customization.
extraTemplateManifests: []
#  - |
#    apiVersion: v1
#    kind: ConfigMap
#    metadata:
#      name: my-config
#    stringData:
#      image_name: {{ .Values.image.repository }}

# Official Valkey Helm Chart configuration
# https://github.com/valkey-io/valkey-helm
valkey:
  enabled: false
  # replicaCount: 1
  #
  # auth:
  #   enabled: false
  #
  # dataStorage:
  #   enabled: false
  #   requestedSize: 2Gi

redis:
  enabled: true
  image:
    registry: docker.io
    repository: redis
    tag: "8.4.0@sha256:3906b477e4b60250660573105110c28bfce93b01243eab37610a484daebceb04"
    pullPolicy: Always
  architecture: cluster
  replicaCount: 3
  clusterReplicaCount: 0
  auth:
    enabled: true
    password: "password"
  persistence:
    enabled: true
    storageClass: ""
    accessMode: ReadWriteOnce
    size: 10Gi
    mountPath: /data
  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "512Mi"

postgresql:
  enabled: true
  image:
    registry: docker.io
    repository: postgres
    tag: "18.1@sha256:38d5c9d522037d8bf0864c9068e4df2f8a60127c6489ab06f98fdeda535560f9"
    imagePullPolicy: Always
  customUser:
    name: "n8n"
    database: "n8n"
    password: "password"
  replicaCount: 1
  persistence:
    enabled: true
    storageClass: ""
    size: 50Gi
    accessModes:
      - ReadWriteOnce